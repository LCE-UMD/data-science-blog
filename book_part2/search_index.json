[
["index.html", "Part II: Statistics Chapter 1 The magic of the central limit theorem: 1.1 Sampling, sampling, sampling…", " Part II: Statistics Chapter 1 The magic of the central limit theorem: 1.1 Sampling, sampling, sampling… As scientists we aim to understand the world around us, not just our immediate environments. In most cases, we don’t have access to populations, for one, because they are… large. For example, if you’re studying expectant mothers, it is virtually impossible to collect data from all of the expectant mothers from around the world. Therefore, we make do with random and representative samples of the population to make generalizations – that is, statements – about the population as a whole. The central limit theorem (CLT) states that the larger the sample size collected, the closer the distribution of the sample means will resemble a normal distribution (bell curve), regardless of the population’s distribution. If you were asleep during your stats class or need a refresher, Khan Academy gives a good introduction to CLT. It’s useful to re-read the statement above, because we’re not saying that we’re assuming that the observations (e.g., one sample of with \\(100\\)) originate from a normal distribution. We’re saying that the distribution of sample means (based on taking the mean of many separate samples that originate from the “parent” distribution) will be normally distributed. That is, provided your sample size is “large enough.” The theorem works “in the limit”, as mathematicians say, but a general rule is that the sample size should be at least \\(30\\) for the CLT to hold for almost any data distribution. And, in practice, it will work on smaller samples if they originate from a population that actually has a normal distribution. Let’s refresh ourselves with a few terms. Variance refers to the amount of variability, or how spread the data are from the mean. The population variance is symbolized as sigma squared, or \\(\\sigma^{2}\\). Because variance is a squared term, we tend to look at the square root of variance, or standard deviation, and the population standard deviation is symbolized as \\(\\sigma\\), or sigma. We know that as the size of the sample increases, the closer the sampling distribution of the sample mean will resemble a bell curve with a mean that approaches the population mean, \\(\\mu\\). How about the standard deviation of the distribution of the sample means? As the sample size increases, the CLT says that the standard deviation will approach \\(\\frac{\\mathbf{\\sigma}}{\\sqrt{n}}\\). Again, this results holds despite the shape of the population distribution. A good source of intuitive discussion on the central limit theorem is Mordkoff, J.T. (2016) The Assumption(s) of Normality. Post originally by Kelly Morrow and edited by L. Pessoa "],
["applying-the-central-limit-theorem.html", "Chapter 2 Applying the Central Limit Theorem 2.1 An example 2.2 Central Limit Theorem 2.3 Samples 2.4 Hypothesis Testing 2.5 The flaw in the z-test", " Chapter 2 Applying the Central Limit Theorem 2.1 An example According the National Center for Health Statistics, the distribution of serum cholesterol levels for 20 to 74-year-old males living in the United States has mean 211 mg/dl, and a standard deviation of 46 mg/dl. We are planning to collect a sample of 25 individuals and measure their cholesterol levels. We are interested in the following about the sample: What is the probability that our sample mean will be above a certain limit, say 230? What is the 95% confidence interval of our sample means? How do these vary as we collect more samples? Does the probability increase or decrease? Does the size of the confidence interval increase or decrease? By what factor does it increase or decrease? Finally, how large would the sample size have to be to ensure a 95% probability that the sample average is within 5 mg/dl of the population mean? How does the Central Limit Theorem (CLT) help us answer these questions? 2.2 Central Limit Theorem Given a population with a finite mean \\(\\mu\\) and a finite non-zero standard deviation \\(\\sigma\\), the distribution of the sample means approaches a normal distribution as the sample size increases. The mean of the sample means is given by \\[\\mu_{\\bar{X}} = \\mu,\\] and the standard deviation of the sample means (also referred to as the standard error of the mean) is given by \\[\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{N}}.\\] For a more precise version of CLT, please refer Wolfram. An important observation is that no assumptions are made about the distribution of the parental population. It could be discrete or continuous, severely skewed, but as long as the mean and standard deviation are finite, CLT holds. To convince yourself of this, please take a look at examples here or use the simulator here. 2.3 Samples But what is a sample? We keep using that word, and its meaning is quite an important concept. A sample is a random draw of size N of data from the parent distribution. We obtain a sample of data every time we randomly draw from what we conceptualize as the population of interest. If the population of interest in every student at UMD, then a random draw is obtained by any mechanism that (truly) randomly draws from it (think of an imaginary lottery machine that, after spinning it, we can obtain one student at a time). And the sample mean? The sample mean is just the average of the measure of interest from the \\(N\\) units that were sampled. So for each mean, we get exactly one sample mean. When we’re thinking about the CLT (what it means), we need to think of repeating this process many times to have multiple sample means. Remember, that each sample has \\(N\\) units. So for each mean, you need to sample a group of size \\(N\\). This is what the CLT talks about. One reason this might appear confusing is that in any one given study, we only sample once (with size \\(N\\)). That’s the world the experimenter lives in (except it she repeats her experiment). But that’s not the world of the CLT, which instead is a world in which we perform an experiment (a sample draw of size \\(N\\)), over and over. And a few more times… Back to the previous questions. To answer them, it is essential to know the sampling distribution, that is, the distribution of the sample mean. Since the standard deviation of the parent population is known, from CLT it follows that the sampling distribution (\\(N=25\\)) has a mean \\(\\mu_{\\bar{X}} = 211\\) mg/dl, and standard deviation (this is called standard error) \\(\\sigma_{\\bar{X}} = \\frac{46}{\\sqrt{25}} = 9.2.\\) Note that the standard error reduces as the number of samples increase by a factor \\(\\sqrt{N}.\\) Our limit, \\(230\\) mg/dl, is therefore \\(\\frac{230 - 211}{9.2} = 2.07\\) standard deviations away from the mean. In other words, the z-score associated with the limit \\(230\\) mg/dl is \\(2.07\\). The answer to our first question is simple the probability that a normally distributed random variable is greater than \\(2.07\\) standard deviations away from the mean = \\(1.9\\%\\) (or \\(0.019\\)). For a normally distributed random variable, 95% of the values lie within 1.96 standard deviations of its mean (on either side). The standard deviation remains the same, \\(9.2\\). Thus, the \\(95\\%\\) confidence interval is simply, 211 - \\(1.96(9.2) = 193.0\\) to \\(211 + 1.96(9.2) = 229.0\\). Suppose we had only \\(10\\) samples. Verify that the new standard error would be \\(14.5\\) and the z-score associated with the limit, \\(230\\) mg/dl, would be \\(1.31\\). The probability of our sample mean being over \\(230\\) would thus be \\(9.6\\%\\) (almost \\(5\\) times higher). On the other hand, our confidence interval would be much larger with \\(10\\) samples; \\((182.5, 239.5)\\). How about if we had \\(50\\) samples? This would result is a narrower confidence interval \\((198.2, 223.8)\\) since the standard error is smaller. To answer our final question, we need \\(1.96\\) standard deviations of the sampling distribution to amount to \\(5\\) mg/dl. Thus, the standard error should be \\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{N}} = 5/1.96.\\) Since, we know \\(\\sigma\\), \\(N = 325.1\\). We would need at least \\(326\\) samples to ensure this confidence interval. 2.4 Hypothesis Testing Thinking along these lines can be used to develop hypothesis tests and understand p-values. We start again with an example: Cystic fibrosis is a genetic disease that affects lung function. Forced vital capacity (FVC) is the volume of air that a person can expel from the lungs in \\(6\\) seconds. It is often used as a marker of the progression of cystic fibrosis. \\(14\\) participants received both a drug and placebo (at different times), and their FVC was measured at the beginning and end of each treatment period. In the study, the mean difference in reduction in FVC (placebo - drug) was \\(137\\), with a sample standard deviation \\(223\\). Does the drug have a significant effect? The null hypothesis is that the drug has no effect, thus the reduction in FVC should be zero. Let’s first find the probability of observing an FVC reduction of greater than \\(137\\) given the null hypothesis. We calculate the standard error based on the \\(14\\) participants as \\(\\sigma_{\\bar{X}} = \\frac{223}{\\sqrt{14}} = 60.\\) The z-score associated with the mean reduction in FVC is given by \\(\\frac{(137 - 0)}{60} = 2.28\\). The probability of observing a value further from the mean by at least \\(2.28\\) standard deviations, which is also the p-value, which is \\(2.2\\%\\) (or \\(0.022\\)). This is a small probability that the drug is having an effect just by chance. Why did we obtain a small probability? Because it’s effect by chance should be zero. But because we’re working with a sample (\\(N=14\\)) that is randomly drawn from the population, the observed mean reduction will fluctuate from sample to sample. Based on data from our sample, the reduction was \\(137\\). But how large is \\(137\\)? We don’t know without some form of calibration, which can be obtained by the information that was given to us: \\(\\sigma_{\\bar{X}}\\). From these data, we can believe that the drug helps prevent deterioration in lung function. At least the data are consistent with this notion in a probabilistic sense. Another way to think about this, it would be somewhat irrational to think that we could obtain the observed result just by chance. Maybe not entirely with a p of basically \\(\\frac{2}{100}\\) but probably with a p of \\(\\frac{1}{1000}\\). But obviously this is somewhat subjective. 2.5 The flaw in the z-test Is the above reasoning correct? To understand this we need to understand the difference between the first and second examples. In our first example, an oracle provided us with the standard deviation of the population. Some all-knowing being told us what the population \\(\\sigma\\) was. But in the second example the standard error was based on the sample. To make the distinction clear, we call the standard error based on sample data \\(s_{\\bar{X}}\\). Important aside: why use the sample and not the population? Populations are essentially Platonic objects. We typically don’t have complete knowledge about the objects we want to study. If we did we wouldn’t need to study them in the first place! So we have to draw samples and do the best based on finite amounts of data. Another way to think about this is that oracles don’t walk around waiting to be interrogated. Maybe they were around in ancient Greece, but not anymore… Gossett (which published under the pseudonym Student) showed that when the standard error is estimated from sample data, the statistic \\(\\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{X}}}\\) is not normal, but follows a t-distribution with \\(N - 1\\) degrees of freedom (df). The t-distribution looks very much like a normal, but has what we call heavier tails, that is, more mass along the tails relative to the normal. Thus, the probability associated with a t-score of \\(2.28\\) with \\(14 - 1\\) degrees of freedom can be calculated to be \\(4\\%\\) (or \\(0.04\\)). The p-value obtained from the z-test (\\(2.2\\%\\)) overstates the evidence against the null hypothesis (this is consistent with the fact that a normal is thinner along the tails than the t-distribution). Examples are borrowed from Introduction to Biostatistics kindly offered by Patrick Breheny at UIowa. Post by Manasij Venkatesh, with edits by L. Pessoa. "],
["bootstrapping.html", "Chapter 3 Bootstrapping", " Chapter 3 Bootstrapping In the real world, there are many physical and financial constraints on gathering a large data set from a population. Take for example the fMRI, an important tool used to collect brain imaging. It would be wonderful to perform scans on millions upon millions of people to understand brains networks that shed light on how the brain functions. But due to this tool being very expensive, it is unfeasible to scan millions of people. So what do researchers do? Since there is a limited funding, researchers collect data from a small number of people compared to the population. A typical fMRI study includes \\(30-60\\) participants and when we compare this number with \\(7\\) billion people, it is minuscule. When we can only scan about \\(60\\) people, we need a way to infer something about a population from the sample we gathered. This is where bootstrapping comes in. Bootstrapping is re-sampling with replacement from one sample that was gathered from a population. The basic idea is that inference about a population from sample data can be modeled by re-sampling the sample data and performing inference about a sample from re-sampled data. We know nothing about the population because all we have is the sample data. A good thing about bootstrapping is its incredible simplicity. It is at its core the same thing as Central limit Theorem. In Central Limit Theorem, where the means of the multiple samples from a population is plotted on a distribution or sample distribution of sample means. In Bootstrapping, the bootstrap sample is drawn repeatedly with replacement from the original sample. The original sample stands for the population. The mean of these bootstrap samples is plotted, which becomes bootstrap distribution. Let’s take an example that I just made up. We are in a very large field where there could be millions of rabbits. We only have enough resources to catch \\(10\\) rabbits. We go to different areas of the field and randomly catch \\(10\\) rabbits. We want to find the weight of rabbits in kilograms and infer this information onto the population. Our sample, \\(N = 10\\), is distributed in the following fashion \\([2, 3, 3, 5, 5, 6, 7, 8, 10, 13]\\) in kg. We want to run the bootstrapping technique with this sample. Remember bootstrapping is basically re-sampling with replacement from the original sample. We are not going into the field and catching \\(10\\) more rabbits to find their weight. We only have the weights of the \\(10\\) rabbits we caught in our original sample. So we run a re-sampling with selecting \\(N\\) number of re-samples. We get the following result \\([5, 5, 7, 3, 3, 3, 13, 10, 5, 2]\\). Let’s call this sample bootstrap sample \\(1\\). The mean of bootstrap sample \\(1\\) is \\(5.6\\). We re-sample again from the original sample and get the following result \\([6, 2, 5, 6, 10, 10 ,5, 13, 2, 2]\\). Lets call this sample bootstrap sample \\(2\\). The mean of bootstrap sample \\(2\\) is \\(6.1\\). We can see that we are treating the original sample as the new “mini-population”. We can re-sample again and again and get a mean value of \\(7.1, 4.5, \\cdots\\) etc. Depending on the computing power we have, we can perform this re-sampling millions of times. We then plot these “bootstrap means” onto a graph and see how the distribution looks like. This distribution of re-sampled means is the bootstrapping distribution. The bootstrapping distribution is used to see whether the sampling distribution is normal rather than hoping that the large sample size we collected is large enough for the central limit theorem to apply. Bootstrap re-sampling is not a substitution of getting a “good” sample. We can go back to the rabbits example. Lets say that we had not gathered the original sample independently. Instead of randomly selecting rabbits, we had only gone to one burrow and gathered sample from a single burrow which only contained \\(10\\) baby rabbits. The weight data would appear as \\([1,1,1,1,1,2,1,1,2,1]\\). The mean would be \\(1.2\\). This is not random sampling and we can run as many bootstrapping re-sampling as we want but it will not magically bring us any closer to the population mean weight of rabbits. Posted by Adnan Rashid "],
["permutation-testing.html", "Chapter 4 Permutation Testing", " Chapter 4 Permutation Testing Similar to the standard independent or unpaired samples t-test, Permutation tests can be used to perform hypothesis testing to test whether the observed mean difference between two groups is statistically significant or not. For example, our research question is to test whether the salaries of male and female employees working at a big multinational company differ? For this, say we collected salary information from a random sample of \\(30\\) male and \\(30\\) female employees (total of \\(60\\) salaries). If we would like to use the permutation testing to test whether the salaries of male and female employees are statistically different, the following steps are involved: Calculate the mean salary difference between male and female sample groups. This is the sample mean difference. Build the permutation distribution for the mean salary difference based on large number of permutation resamples generated without replacement. In order to perform hypothesis testing, we should resample in a manner that is consistent with the null hypothesis (and with the study design). In this case, null hypothesis is that there is no salary difference between male and female populations which implies that gender doesn’t play a role in employee’s salaries. So, to create a permutation resample, we randomly assign \\(30\\) out of \\(60\\) salaries to male group and remaining \\(30\\) salaries to female group. Then, we calculate the mean salary difference between two groups for this permutation resample. We repeat this resampling procedure without replacement thousands of times and calculate mean salary difference for each permutation resample. Finally, plotting the distribution of the mean salary difference of all permutation resamples gives us the sampling distribution under the null hypothesis. Calculate the p-value by finding the observed sample mean difference on the permutation distribution. In other words, find the proportion of samples in the permutation distribution that are at or more extreme than the sample mean difference calculated in step \\(1\\). If the observed p-value is low (i.e., if the sample mean difference fell on tails of the permutation distribution), then we can infer that the observed mean salary difference is less likely happened by chance and hence conclude that the population means of male and female employee salaries are statistically different. Compared to the standard t test, permutation tests are accurate even when the population distributions are not normal. However, under the null hypothesis, permutation tests require the two populations to have identical distributions with same means, same shapes and same variability (how can we test this?). Posted by Srikanth Padmala. "],
["the-beauty-of-programming-via-probability-distributions-and-estimating-them.html", "Chapter 5 The beauty of programming via probability distributions (and estimating them)", " Chapter 5 The beauty of programming via probability distributions (and estimating them) What does the title even mean? Say you encountered this in a statistics text: \\[ x \\sim N(0, 1) \\] which just says that the random variable \\(x\\) is normally distributed with \\(0\\) mean and variance \\(1\\). If you write that in your program, \\(x\\) will automatically be distributed that way and draws from that distribution will be drawn when \\(x\\) is needed elsewhere. Here’s a more complex model: \\[ z_{lk} = p_{l} + \\epsilon_{l, k}, k = 1, 2, \\cdots, n \\] This is ugly, but \\({lk}\\) and \\({l}\\) are just subscripts, that is, indeces. What does this mean? The varible \\(z\\) is estimated as an additive contribution of a population effect of the variable that you are interested in \\(p\\) (for example, the experimental condition that you manipulated \\(l\\); “ell”) and \\(\\epsilon_{lk}\\) is the deviation (or effect) of subject \\(k\\) on condition \\(l\\). What does this mean? If you know what the Stroop effect is: \\(p_l\\) could be the contribution (effect) of an incongruent condition. And \\(\\epsilon_{l,k}\\) the contribution (effect) of subject \\(k\\), that is, sort of the average response time across all conditions for that person (if I’m fast in general, or slow in general). So what the experimental condition (incongruent trials) does is a deviation from my average reaction time based on the condition. By including both a fixed effect p (the conditions of your experiment; they are fixed, right?) and the random effect (the one that you draw randomly), that is, participants, you have a mixed-effcts model And you’re done! Or not far from it actually (so close that it’s scary). If you don’t know what the Stroop effect is, it doesn’t matter (but look it up). The model is general and that’s what math is for, right? Well, this was a little longer than I thought. "]
]
