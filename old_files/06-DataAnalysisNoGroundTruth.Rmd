---
title: 'Blog: How can we do data analysis when there's no "ground truth"?'
output: html_document
---

# How can we do data analysis when there''s no "ground truth"?

We often investigate novel imaging datasets with sophisticated algorithms. These might be something that we just developed or some technique that exists in the literature and has been applied in other domains. In any case, it's something that isn't trivial and will involved quite a bit of data processing or manipulation.

You get a new result, now what? Maybe the result makes some sense, or maybe it's not entirely clear. Can you trust the results? If it the results make sense, should you trust the results?

The answer is **no, don't trust it** – yet.

There are more ways to get something wrong than to get it right. So just because of this it will be quite possible that something went wrong. Not just in terms of programming but also **conceptually**. Maybe you thought what you were doing made sense, but it actually doesn't and you haven't realized it. Maybe it will take quite some time for you to understand **your method** more deeply.

What to do?

First, apply your method to something that is **simple** and that you believe you can interpret. Try to **break** your method to understand it better. Do this for a good while, by the way.

Next, apply it to something that is **not simple** but that is well understood, or relatively well understood. Let me give you an example here. Suppose you want to study something novel and challenging, like our `emoprox I/II` data. You apply your algorithm and get some result. Maybe you don't even have much experience with brain data, so interpreting will be quite hard. So, now apply it to `HCP data`, for example, which has been studied extensively in many different ways. So by usign this **better understood** dataset and now understanding your method a little better (and fixing any implementation and hopefully minor conceptual mistakes), you can now return to your original problem of interest.

There's still a **lot to understand** but at least now you know something about the variability of the measures that you're using, how they might be distributed, and so on.

A lot of research is done – obviously – without any **ground truth**. That is an immense challenge and even very skilled people technically struggle when studying new problems. That's because both the data **and** their methods are not fully understood. This is a perfect recipe for things to go wrong, at least partly wrong. So one has to incrementally **bootstrap** one's understanding of both the data and the method along the ways outlined here.

Obviously, the example above is arbitrary (not entirely by the way). Maybe you are studying `HCP data` for the first time yourself with your method. So pick something simpler than `HCP` and apply the logic above. If you don't know what data to pick, or it's not obvious what could be used, an excellent choice is to **create synthetic data** yourself. You should contruct data with certain properties and understand how your method/measures behaves. Again, think of the distribution of results that you get; how do they vary? Synthetic data is seldom used, but it's hugely useful. So think of how to structure your workflow so as to incorporate that in a more seamless manner.

I'm going to stop here. There's a lot more to discuss about this, but I wanted to post this simple version first. Maybe one day I'll extend it based on questions or suggestions.