[
["index.html", "Data Science Chapter 1 Book Purpose", " Data Science Luiz Pessoa 2019-02-02 Chapter 1 Book Purpose The purpose of this book is to teach concepts in Data Science, Statistics, and fMRI Research. "],
["the-magic-of-the-central-limit-theorem.html", "Chapter 2 The magic of the central limit theorem: 2.1 Sampling, sampling, sampling…", " Chapter 2 The magic of the central limit theorem: 2.1 Sampling, sampling, sampling… As scientists we aim to understand the world around us, not just our immediate environments. In most cases, we don’t have access to populations, for one, because they are… large. For example, if you’re studying expectant mothers, it is virtually impossible to collect data from all of the expectant mothers from around the world. Therefore, we make do with random and representative samples of the population to make generalizations – that is, statements – about the population as a whole. The central limit theorem (CLT) states that the larger the sample size collected, the closer the distribution of the sample means will resemble a normal distribution (bell curve), regardless of the population’s distribution. If you were asleep during your stats class or need a refresher, Khan Academy gives a good introduction to CLT. It’s useful to re-read the statement above, because we’re not saying that we’re assuming that the observations (e.g., one sample of with \\(100\\)) originate from a normal distribution. We’re saying that the distribution of sample means (based on taking the mean of many separate samples that originate from the “parent” distribution) will be normally distributed. That is, provided your sample size is “large enough.” The theorem works “in the limit”, as mathematicians say, but a general rule is that the sample size should be at least \\(30\\) for the CLT to hold for almost any data distribution. And, in practice, it will work on smaller samples if they originate from a population that actually has a normal distribution. Let’s refresh ourselves with a few terms. Variance refers to the amount of variability, or how spread the data are from the mean. The population variance is symbolized as sigma squared, or \\(\\sigma^{2}\\). Because variance is a squared term, we tend to look at the square root of variance, or standard deviation, and the population standard deviation is symbolized as \\(\\sigma\\), or sigma. We know that as the size of the sample increases, the closer the sampling distribution of the sample mean will resemble a bell curve with a mean that approaches the population mean, \\(\\mu\\). How about the standard deviation of the distribution of the sample means? As the sample size increases, the CLT says that the standard deviation will approach \\(\\frac{\\mathbf{\\sigma}}{\\sqrt{n}}\\). Again, this results holds despite the shape of the population distribution. A good source of intuitive discussion on the central limit theorem is Mordkoff, J.T. (2016) The Assumption(s) of Normality. Post originally by Kelly Morrow and edited by L. Pessoa "],
["applying-the-central-limit-theorem.html", "Chapter 3 Applying the Central Limit Theorem 3.1 An example 3.2 Central Limit Theorem 3.3 Samples 3.4 Hypothesis Testing 3.5 The flaw in the z-test", " Chapter 3 Applying the Central Limit Theorem 3.1 An example According the National Center for Health Statistics, the distribution of serum cholesterol levels for 20 to 74-year-old males living in the United States has mean 211 mg/dl, and a standard deviation of 46 mg/dl. We are planning to collect a sample of 25 individuals and measure their cholesterol levels. We are interested in the following about the sample: What is the probability that our sample mean will be above a certain limit, say 230? What is the 95% confidence interval of our sample means? How do these vary as we collect more samples? Does the probability increase or decrease? Does the size of the confidence interval increase or decrease? By what factor does it increase or decrease? Finally, how large would the sample size have to be to ensure a 95% probability that the sample average is within 5 mg/dl of the population mean? How does the Central Limit Theorem (CLT) help us answer these questions? 3.2 Central Limit Theorem Given a population with a finite mean \\(\\mu\\) and a finite non-zero standard deviation \\(\\sigma\\), the distribution of the sample means approaches a normal distribution as the sample size increases. The mean of the sample means is given by \\[\\mu_{\\bar{X}} = \\mu,\\] and the standard deviation of the sample means (also referred to as the standard error of the mean) is given by \\[\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{N}}.\\] For a more precise version of CLT, please refer Wolfram. An important observation is that no assumptions are made about the distribution of the parental population. It could be discrete or continuous, severely skewed, but as long as the mean and standard deviation are finite, CLT holds. To convince yourself of this, please take a look at examples here or use the simulator here. 3.3 Samples But what is a sample? We keep using that word, and its meaning is quite an important concept. A sample is a random draw of size N of data from the parent distribution. We obtain a sample of data every time we randomly draw from what we conceptualize as the population of interest. If the population of interest in every student at UMD, then a random draw is obtained by any mechanism that (truly) randomly draws from it (think of an imaginary lottery machine that, after spinning it, we can obtain one student at a time). And the sample mean? The sample mean is just the average of the measure of interest from the \\(N\\) units that were sampled. So for each mean, we get exactly one sample mean. When we’re thinking about the CLT (what it means), we need to think of repeating this process many times to have multiple sample means. Remember, that each sample has \\(N\\) units. So for each mean, you need to sample a group of size \\(N\\). This is what the CLT talks about. One reason this might appear confusing is that in any one given study, we only sample once (with size \\(N\\)). That’s the world the experimenter lives in (except it she repeats her experiment). But that’s not the world of the CLT, which instead is a world in which we perform an experiment (a sample draw of size \\(N\\)), over and over. And a few more times… Back to the previous questions. To answer them, it is essential to know the sampling distribution, that is, the distribution of the sample mean. Since the standard deviation of the parent population is known, from CLT it follows that the sampling distribution (\\(N=25\\)) has a mean \\(\\mu_{\\bar{X}} = 211\\) mg/dl, and standard deviation (this is called standard error) \\(\\sigma_{\\bar{X}} = \\frac{46}{\\sqrt{25}} = 9.2.\\) Note that the standard error reduces as the number of samples increase by a factor \\(\\sqrt{N}.\\) Our limit, \\(230\\) mg/dl, is therefore \\(\\frac{230 - 211}{9.2} = 2.07\\) standard deviations away from the mean. In other words, the z-score associated with the limit \\(230\\) mg/dl is \\(2.07\\). The answer to our first question is simple the probability that a normally distributed random variable is greater than \\(2.07\\) standard deviations away from the mean = \\(1.9\\%\\) (or \\(0.019\\)). For a normally distributed random variable, 95% of the values lie within 1.96 standard deviations of its mean (on either side). The standard deviation remains the same, \\(9.2\\). Thus, the \\(95\\%\\) confidence interval is simply, 211 - \\(1.96(9.2) = 193.0\\) to \\(211 + 1.96(9.2) = 229.0\\). Suppose we had only \\(10\\) samples. Verify that the new standard error would be \\(14.5\\) and the z-score associated with the limit, \\(230\\) mg/dl, would be \\(1.31\\). The probability of our sample mean being over \\(230\\) would thus be \\(9.6\\%\\) (almost \\(5\\) times higher). On the other hand, our confidence interval would be much larger with \\(10\\) samples; \\((182.5, 239.5)\\). How about if we had \\(50\\) samples? This would result is a narrower confidence interval \\((198.2, 223.8)\\) since the standard error is smaller. To answer our final question, we need \\(1.96\\) standard deviations of the sampling distribution to amount to \\(5\\) mg/dl. Thus, the standard error should be \\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{N}} = 5/1.96.\\) Since, we know \\(\\sigma\\), \\(N = 325.1\\). We would need at least \\(326\\) samples to ensure this confidence interval. 3.4 Hypothesis Testing Thinking along these lines can be used to develop hypothesis tests and understand p-values. We start again with an example: Cystic fibrosis is a genetic disease that affects lung function. Forced vital capacity (FVC) is the volume of air that a person can expel from the lungs in \\(6\\) seconds. It is often used as a marker of the progression of cystic fibrosis. \\(14\\) participants received both a drug and placebo (at different times), and their FVC was measured at the beginning and end of each treatment period. In the study, the mean difference in reduction in FVC (placebo - drug) was \\(137\\), with a sample standard deviation \\(223\\). Does the drug have a significant effect? The null hypothesis is that the drug has no effect, thus the reduction in FVC should be zero. Let’s first find the probability of observing an FVC reduction of greater than \\(137\\) given the null hypothesis. We calculate the standard error based on the \\(14\\) participants as \\(\\sigma_{\\bar{X}} = \\frac{223}{\\sqrt{14}} = 60.\\) The z-score associated with the mean reduction in FVC is given by \\(\\frac{(137 - 0)}{60} = 2.28\\). The probability of observing a value further from the mean by at least \\(2.28\\) standard deviations, which is also the p-value, which is \\(2.2\\%\\) (or \\(0.022\\)). This is a small probability that the drug is having an effect just by chance. Why did we obtain a small probability? Because it’s effect by chance should be zero. But because we’re working with a sample (\\(N=14\\)) that is randomly drawn from the population, the observed mean reduction will fluctuate from sample to sample. Based on data from our sample, the reduction was \\(137\\). But how large is \\(137\\)? We don’t know without some form of calibration, which can be obtained by the information that was given to us: \\(\\sigma_{\\bar{X}}\\). From these data, we can believe that the drug helps prevent deterioration in lung function. At least the data are consistent with this notion in a probabilistic sense. Another way to think about this, it would be somewhat irrational to think that we could obtain the observed result just by chance. Maybe not entirely with a p of basically \\(\\frac{2}{100}\\) but probably with a p of \\(\\frac{1}{1000}\\). But obviously this is somewhat subjective. 3.5 The flaw in the z-test Is the above reasoning correct? To understand this we need to understand the difference between the first and second examples. In our first example, an oracle provided us with the standard deviation of the population. Some all-knowing being told us what the population \\(\\sigma\\) was. But in the second example the standard error was based on the sample. To make the distinction clear, we call the standard error based on sample data \\(s_{\\bar{X}}\\). Important aside: why use the sample and not the population? Populations are essentially Platonic objects. We typically don’t have complete knowledge about the objects we want to study. If we did we wouldn’t need to study them in the first place! So we have to draw samples and do the best based on finite amounts of data. Another way to think about this is that oracles don’t walk around waiting to be interrogated. Maybe they were around in ancient Greece, but not anymore… Gossett (which published under the pseudonym Student) showed that when the standard error is estimated from sample data, the statistic \\(\\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{X}}}\\) is not normal, but follows a t-distribution with \\(N - 1\\) degrees of freedom (df). The t-distribution looks very much like a normal, but has what we call heavier tails, that is, more mass along the tails relative to the normal. Thus, the probability associated with a t-score of \\(2.28\\) with \\(14 - 1\\) degrees of freedom can be calculated to be \\(4\\%\\) (or \\(0.04\\)). The p-value obtained from the z-test (\\(2.2\\%\\)) overstates the evidence against the null hypothesis (this is consistent with the fact that a normal is thinner along the tails than the t-distribution). Examples are borrowed from Introduction to Biostatistics kindly offered by Patrick Breheny at UIowa. Post by Manasij Venkatesh, with edits by L. Pessoa. "],
["bootstrapping.html", "Chapter 4 Bootstrapping", " Chapter 4 Bootstrapping In the real world, there are many physical and financial constraints on gathering a large data set from a population. Take for example the fMRI, an important tool used to collect brain imaging. It would be wonderful to perform scans on millions upon millions of people to understand brains networks that shed light on how the brain functions. But due to this tool being very expensive, it is unfeasible to scan millions of people. So what do researchers do? Since there is a limited funding, researchers collect data from a small number of people compared to the population. A typical fMRI study includes \\(30-60\\) participants and when we compare this number with \\(7\\) billion people, it is minuscule. When we can only scan about \\(60\\) people, we need a way to infer something about a population from the sample we gathered. This is where bootstrapping comes in. Bootstrapping is re-sampling with replacement from one sample that was gathered from a population. The basic idea is that inference about a population from sample data can be modeled by re-sampling the sample data and performing inference about a sample from re-sampled data. We know nothing about the population because all we have is the sample data. A good thing about bootstrapping is its incredible simplicity. It is at its core the same thing as Central limit Theorem. In Central Limit Theorem, where the means of the multiple samples from a population is plotted on a distribution or sample distribution of sample means. In Bootstrapping, the bootstrap sample is drawn repeatedly with replacement from the original sample. The original sample stands for the population. The mean of these bootstrap samples is plotted, which becomes bootstrap distribution. Let’s take an example that I just made up. We are in a very large field where there could be millions of rabbits. We only have enough resources to catch \\(10\\) rabbits. We go to different areas of the field and randomly catch \\(10\\) rabbits. We want to find the weight of rabbits in kilograms and infer this information onto the population. Our sample, \\(N = 10\\), is distributed in the following fashion \\([2, 3, 3, 5, 5, 6, 7, 8, 10, 13]\\) in kg. We want to run the bootstrapping technique with this sample. Remember bootstrapping is basically re-sampling with replacement from the original sample. We are not going into the field and catching \\(10\\) more rabbits to find their weight. We only have the weights of the \\(10\\) rabbits we caught in our original sample. So we run a re-sampling with selecting \\(N\\) number of re-samples. We get the following result \\([5, 5, 7, 3, 3, 3, 13, 10, 5, 2]\\). Let’s call this sample bootstrap sample \\(1\\). The mean of bootstrap sample \\(1\\) is \\(5.6\\). We re-sample again from the original sample and get the following result \\([6, 2, 5, 6, 10, 10 ,5, 13, 2, 2]\\). Lets call this sample bootstrap sample \\(2\\). The mean of bootstrap sample \\(2\\) is \\(6.1\\). We can see that we are treating the original sample as the new “mini-population”. We can re-sample again and again and get a mean value of \\(7.1, 4.5, \\cdots\\) etc. Depending on the computing power we have, we can perform this re-sampling millions of times. We then plot these “bootstrap means” onto a graph and see how the distribution looks like. This distribution of re-sampled means is the bootstrapping distribution. The bootstrapping distribution is used to see whether the sampling distribution is normal rather than hoping that the large sample size we collected is large enough for the central limit theorem to apply. Bootstrap re-sampling is not a substitution of getting a “good” sample. We can go back to the rabbits example. Lets say that we had not gathered the original sample independently. Instead of randomly selecting rabbits, we had only gone to one burrow and gathered sample from a single burrow which only contained \\(10\\) baby rabbits. The weight data would appear as \\([1,1,1,1,1,2,1,1,2,1]\\). The mean would be \\(1.2\\). This is not random sampling and we can run as many bootstrapping re-sampling as we want but it will not magically bring us any closer to the population mean weight of rabbits. Posted by Adnan Rashid "],
["permutation-testing.html", "Chapter 5 Permutation Testing", " Chapter 5 Permutation Testing Similar to the standard independent or unpaired samples t-test, Permutation tests can be used to perform hypothesis testing to test whether the observed mean difference between two groups is statistically significant or not. For example, our research question is to test whether the salaries of male and female employees working at a big multinational company differ? For this, say we collected salary information from a random sample of \\(30\\) male and \\(30\\) female employees (total of \\(60\\) salaries). If we would like to use the permutation testing to test whether the salaries of male and female employees are statistically different, the following steps are involved: Calculate the mean salary difference between male and female sample groups. This is the sample mean difference. Build the permutation distribution for the mean salary difference based on large number of permutation resamples generated without replacement. In order to perform hypothesis testing, we should resample in a manner that is consistent with the null hypothesis (and with the study design). In this case, null hypothesis is that there is no salary difference between male and female populations which implies that gender doesn’t play a role in employee’s salaries. So, to create a permutation resample, we randomly assign \\(30\\) out of \\(60\\) salaries to male group and remaining \\(30\\) salaries to female group. Then, we calculate the mean salary difference between two groups for this permutation resample. We repeat this resampling procedure without replacement thousands of times and calculate mean salary difference for each permutation resample. Finally, plotting the distribution of the mean salary difference of all permutation resamples gives us the sampling distribution under the null hypothesis. Calculate the p-value by finding the observed sample mean difference on the permutation distribution. In other words, find the proportion of samples in the permutation distribution that are at or more extreme than the sample mean difference calculated in step \\(1\\). If the observed p-value is low (i.e., if the sample mean difference fell on tails of the permutation distribution), then we can infer that the observed mean salary difference is less likely happened by chance and hence conclude that the population means of male and female employee salaries are statistically different. Compared to the standard t test, permutation tests are accurate even when the population distributions are not normal. However, under the null hypothesis, permutation tests require the two populations to have identical distributions with same means, same shapes and same variability (how can we test this?). Posted by Srikanth Padmala. "],
["multiple-regression-as-you-never-seen-it-before-or-the-case-for-the-importance-of-the-assumption-of-independence-.html", "Chapter 6 Multiple regression as you never seen it before. Or the case for the importance of the assumption of independence. 6.1 House price prediction", " Chapter 6 Multiple regression as you never seen it before. Or the case for the importance of the assumption of independence. 6.1 House price prediction Imagine the following scenario, you work for a real estate agency and new house has just been made available on the market, how would you go about naming the price for that house? Luckily in this scenario you are not just any other real estate, but a statistics loving, extra diligent one. As such you have saved on record all the information about houses sold in your area for the past \\(10\\) years. In your archives you have recorded the size of each house (in square feet), the number of bedrooms it has, the average income in the respective neighborhood, a subjective rating of how appealing the house is, the year in which it was sold, and how much it was sold for. How can you use all that information from your archives to best predict the value of the new house? Price(Y) # of bedrooms Average income of the neighborhood Rating Year Area/sqf … House tax rate of this region 190,239 4 40,000 2.7 2012 2,642 … 1.08% … … … … … … … … 230,232 3 63,298 4.8 2016 1,324 … 2.01% This problem could be mathematically formulated as following problem: Given a set of variables \\(X = (x_1, x_2, \\cdots , x_n)\\) and another variable \\(Y\\), try to find the “relationship” between \\(X\\) and \\(Y\\). In the house prediction problem above, \\(x1\\) could be total number of bedrooms, \\(x2\\) could be the average income of the neighborhood, xn could be house tax rate of this region and \\(Y\\) could be the deal price at a certain year. Since we want to predict the price of a house as precise as possible, the “relationship” here could be simply interpreted as to find out the smallest difference between the predicted price and the deal price. So we can further formularized the above problem as: Given a set of variables \\(X = [x1, x2, \\cdots , xn]\\) and another variable \\(Y\\), try to find a function \\(f:\\mathcal{R}^n \\rightarrow \\mathcal{R}\\) s.t “difference” between \\(f(X)\\) and \\(Y\\) is minimized. Here, a problem might arise, how can one choose a proper function \\(f(X)\\) ? A naive thought is to simply assume a linear relationship between \\(X\\) and \\(Y\\), i.e. \\[ f(X)=\\sum{}{}\\alpha_i x_i+ \\alpha_0 \\] This linear assumption, even though simple enough, turns out to be very powerful and effective in solving many problems. Given this assumption, now the problem becomes: Given a set of variables \\(X = [x1, x2, \\cdots , xn]\\) and another variable \\(Y\\), use data collected for variable \\(X\\) and \\(Y\\) to determine a set of values of coefficients \\(\\alpha_0, \\alpha_1, \\cdots , \\alpha_n\\) s.t “difference” between \\(f(X)\\) and \\(Y\\) is minimized. The problem we derived from the perspective of optimization is categorized as least-squares problem: \\[\\underset{x}{min}\\quad f_0(x)=\\Vert Ax- b\\Vert_2^2\\] It could be solved analytically with a solution of \\(x= (A^{\\top}A)^{-1} A^{\\top} b\\). Also there are many efficient algorithms available to avoid the expensive computation of inverse of matrix. Most people stop their analysis here when they happily got their final results: a set of coefficient values. Some might even use software packages as some magical black box that could produce a set of coefficient that could linearly fit the data. Known as multiple regression, statisticians would often use the aforementioned analysis to learn about the relationship between several independent or predictor variables and a dependent or criterion variable. So, if we go back to our house price prediction example. The relationships between the different predicting variables and the criterion can be described like this: \\[Price(Y) = 4.7 * \\text{# of bedrooms} + 0.27* \\text{Average income of the neighborhood} + 0.46 * \\text{Rating}\\\\ + 0.2 * \\text{Year} + 0.1 * \\text{Area} + 0.24 * \\text{House tax rate of this region}\\] What this equation is telling us is that when all other variables are being held as consents, every \\(4.7\\) rooms added to the house results in a one-unit change in price of the house; let us say an added \\(100,000\\) to the house total price. Seeing this equation, our agent might reach the conclusion that due to the large coefficient the factor most significantly effecting the house prices in the area is the number of rooms that they have (when compared to the tiny \\(0.1\\) coefficient of the house area for example). The effect of number of room is so central that for an increase of \\(4.7\\) rooms drives the price of the house by another \\(100,000\\$\\). This conclusion would be very wrong, and for a couple of reasons, we will call them the problems of collinearity and scaling. A house’s price is mainly determined by number of bedrooms but not determined by the house area? Who will design a house with many bedrooms but small area? In fact, how can the numbers of room grow without it affecting the area of the house? Can infect one factor in our equation change without it affecting other factors? Of the two problems, that of scaling is simpler, and easier to resolve. A factor’s coefficient is a function of that factor’s range, so for large ranged variables the coefficient before it is likely to get a small value. That small value however, does NOT indicate that it has less impact in determining the value of \\(Y\\). To avoid this problem, most data scientists tend to normalize data before conducting the regression analysis so that values of variable are in similar range. The other problem, collinearity, refers to a situation when one of our predictor variables can be almost entirely predicted by one or a few of the other predicting variables. In our example, it is easy to see how, in many cases, the number of room it has could predict a house’s area. Similarly, if that region house taxation were based on its square feet size, the variable area would entirely predict that of tax. When two or more variables are highly correlated, they are in affect explaining the same phenomenon, and thus redundant. It is therefore a good practice to test for collinearity between the variables prior to designing the regression model. However, even if variables are not entirely collinear, that does not mean we can treat them as orthogonal. If we go back to the verbal explanation of the regression equation, it is important to pay careful attention to the following part “when all other variables are being held as consents…” it means that we can refer to the unique effect a single variable if, and only if, changing that variable has no effect on the other variables. If it does, the equation should be treated as a whole. "],
["how-can-we-do-data-analysis-when-theres-no-ground-truth.html", "Chapter 7 How can we do data analysis when there’’s no “ground truth”?", " Chapter 7 How can we do data analysis when there’’s no “ground truth”? We often investigate novel imaging datasets with sophisticated algorithms. These might be something that we just developed or some technique that exists in the literature and has been applied in other domains. In any case, it’s something that isn’t trivial and will involved quite a bit of data processing or manipulation. You get a new result, now what? Maybe the result makes some sense, or maybe it’s not entirely clear. Can you trust the results? If it the results make sense, should you trust the results? The answer is no, don’t trust it – yet. There are more ways to get something wrong than to get it right. So just because of this it will be quite possible that something went wrong. Not just in terms of programming but also conceptually. Maybe you thought what you were doing made sense, but it actually doesn’t and you haven’t realized it. Maybe it will take quite some time for you to understand your method more deeply. What to do? First, apply your method to something that is simple and that you believe you can interpret. Try to break your method to understand it better. Do this for a good while, by the way. Next, apply it to something that is not simple but that is well understood, or relatively well understood. Let me give you an example here. Suppose you want to study something novel and challenging, like our emoprox I/II data. You apply your algorithm and get some result. Maybe you don’t even have much experience with brain data, so interpreting will be quite hard. So, now apply it to HCP data, for example, which has been studied extensively in many different ways. So by usign this better understood dataset and now understanding your method a little better (and fixing any implementation and hopefully minor conceptual mistakes), you can now return to your original problem of interest. There’s still a lot to understand but at least now you know something about the variability of the measures that you’re using, how they might be distributed, and so on. A lot of research is done – obviously – without any ground truth. That is an immense challenge and even very skilled people technically struggle when studying new problems. That’s because both the data and their methods are not fully understood. This is a perfect recipe for things to go wrong, at least partly wrong. So one has to incrementally bootstrap one’s understanding of both the data and the method along the ways outlined here. Obviously, the example above is arbitrary (not entirely by the way). Maybe you are studying HCP data for the first time yourself with your method. So pick something simpler than HCP and apply the logic above. If you don’t know what data to pick, or it’s not obvious what could be used, an excellent choice is to create synthetic data yourself. You should contruct data with certain properties and understand how your method/measures behaves. Again, think of the distribution of results that you get; how do they vary? Synthetic data is seldom used, but it’s hugely useful. So think of how to structure your workflow so as to incorporate that in a more seamless manner. I’m going to stop here. There’s a lot more to discuss about this, but I wanted to post this simple version first. Maybe one day I’ll extend it based on questions or suggestions. "],
["the-beauty-of-programming-via-probability-distributions-and-estimating-them.html", "Chapter 8 The beauty of programming via probability distributions (and estimating them)", " Chapter 8 The beauty of programming via probability distributions (and estimating them) What does the title even mean? Say you encountered this in a statistics text: \\[ x \\sim N(0, 1) \\] which just says that the random variable \\(x\\) is normally distributed with \\(0\\) mean and variance \\(1\\). If you write that in your program, \\(x\\) will automatically be distributed that way and draws from that distribution will be drawn when \\(x\\) is needed elsewhere. Here’s a more complex model: \\[ z_{lk} = p_{l} + \\epsilon_{l, k}, k = 1, 2, \\cdots, n \\] This is ugly, but \\({lk}\\) and \\({l}\\) are just subscripts, that is, indeces. What does this mean? The varible \\(z\\) is estimated as an additive contribution of a population effect of the variable that you are interested in \\(p\\) (for example, the experimental condition that you manipulated \\(l\\); “ell”) and \\(\\epsilon_{lk}\\) is the deviation (or effect) of subject \\(k\\) on condition \\(l\\). What does this mean? If you know what the Stroop effect is: \\(p_l\\) could be the contribution (effect) of an incongruent condition. And \\(\\epsilon_{l,k}\\) the contribution (effect) of subject \\(k\\), that is, sort of the average response time across all conditions for that person (if I’m fast in general, or slow in general). So what the experimental condition (incongruent trials) does is a deviation from my average reaction time based on the condition. By including both a fixed effect p (the conditions of your experiment; they are fixed, right?) and the random effect (the one that you draw randomly), that is, participants, you have a mixed-effcts model And you’re done! Or not far from it actually (so close that it’s scary). If you don’t know what the Stroop effect is, it doesn’t matter (but look it up). The model is general and that’s what math is for, right? Well, this was a little longer than I thought. "],
["finishingsubmitting-a-paper-in-the-lab.html", "Chapter 9 Finishing/submitting a paper in the lab", " Chapter 9 Finishing/submitting a paper in the lab lab procedures etc As I’ve discussed recently, from now on we need to archive data when a paper is submitted for publication. At a minimum the following is required (and will be updated as we converge on a more established procedure): 1. File the latest version of the paper in bioRxiv or most relevant repository and make sure that the lab’s website has a link to that version; 2. File all data (fMRI and behavior) in our system (not in a personal account/directory); 3. Archive all data in OpenfMRI or other acceptable public format; 4. File all the processing programs, including versions used; 5. File all illustration figures; 6. File all data figures, including scripts/programs used to generate them. In case the figure involved manual decisions (selection of slices, etc.) this has to be thoroughly documented. Item \\(\\textbf{#6}\\) is particularly important and needs executable by someone other than yourself. In a typical case, an RA will work with you on this. In other words, it no longer suffices to have “all the data backed up”. We need to collectively work on this so as to attain better levels of reproducibility: can someone actually reproduce the work that you did? Finally, not that while before this was encouraged, now it is absolutely mandatory and part of working in the lab. Science is moving fast and we’ve been lagging behind for too long. "],
["understanding-data-overfitting-and-p-values.html", "Chapter 10 Understanding data, overfitting, and p values", " Chapter 10 Understanding data, overfitting, and p values conceptual statistics machine learning cross validation If you’re an engineer you might not be very familar with statistical inference and p values and the point of it all might be a little mysterious. At the same time, you might be very familiar with the issue of overfitting data and using something like training and test sets, cross-validation, and other schemes to investigate data. A neuroscientist will be less familar with those instead. But it turns out that they are closely related. This post will be short and elaborated later (hopefully). Suppose you’re an engineer and plot the data and get some 2-D data scatter plot. Then you decide to fit the data with a linear model (just fitting a line). But you also try a quadratic fit, a cubic fit, and even some fancy new spline method just published. It makes sense to try several methods because you don’t know the data well (you’re just studying it now). You then summarize how all of the methods behave, how well they fit the data. The paradox is that you can’t do that in an experimental science situation. You have to decide in advance which method you want to use and only then apply it to the data. But why? Isn’t this too limiting? The reason (this needs to be short) is that when you analyze a new dataset you usually don’t have so much data that you can try a lot of methods, pick what you like most, and then apply it to the data that you really care about. You don’t have data available like that. Because of this, if you try multiple methods you will be reporting what works best for your specific dataset which may generalize very poorly to new datasets. And what you want in general is to be able to generalize. In the case of fit example above, perhaps the spline fit explains most of the variance in the data. But then it could be really overfitting your data and generalize very poorly to new datasets. You could find this out by testing it in a new dataset, that’s easy enough. The problem is that you don’t have data to test it with, so you’re stuck. Data are so difficult to acquire in general that this approach is usually not possible. (HCP data and other large datasets are helpful in this respect, of course.) Bottom line: you have to find some clever way to choose all your analyses in advance. Maybe you can use similar data, maybe simulated data, or just parsimony – the simplest model possible, or the fewest assumptions. To conclude, data analysis in an experimental setting is not a report of what happens to your data when you apply the set of most common types of fits/analyses available. It’s what happens when you apply one type of analysis to one dataset that is supposed to be representative of data in general. Note 1: Of course, it’s always possible to perform exploratory data analysis in the context of methodological studies, but that’s different. Note 2: I never got to how this is related to p values and statistical inference. As it’s commonly said, that’s left as an exercise for the reader… "],
["interpreting-results-humility-pays-off.html", "Chapter 11 Interpreting results: humility pays off", " Chapter 11 Interpreting results: humility pays off In everyone’s research, programming mistakes (bugs) happen with higher frequency than we would want. This is inevitable given that programming is an inexact science – and it takes long time to get good at it. Not to mention putting together code that is partly yours and uses multiple functions developed by others. It becomes criticial then to do the following: Carefully inspect and critically evaluate your results. I’ve noticed that I often detect problems when they should have been spotted by others. Sometimes the detection is rather easy, which still surprises me. This leads me to think that learning to be a good researcher depends on working hard on the statement of the previous paragraph. Let me give a concrete example, though the example itself is not important. Suppose you’re learning a new technique, something like network/graph theory. There are a dozen basic measures (efficiency, modularity, characteristic path length, etc.), and you’re learning about them. Then you compute these measures on a data sample, one measure per individual (and for each condition). How are the measures similar or different? You then compute the correlation between the measures across participants. What would it mean for the correlation between two measures (say, characteristic path length and efficiency for example) to be exactly 1? How about 0? Two different measures cannot be correlated 100% unless they are mathematically and implementationally the exact same thing. In a real sample, they also cannot be correlated 0%, as sampling variability and noise will always produce a correlation that is different from zero, even if very small. So if you observed these values it should be a very strong indication that something is wrong (with p approaching 1…). Figure 11.1: Correlation matrix. The diagonals should be \\(1.0\\) by definition, but can any off-diagonal assume \\(1.0\\)? What if you observed the correlation between two measures as \\(0.99\\)? Could this happen? Figure 11.2: Correlation scatter. The two measures are correlated \\(0.9983\\). Logically speaking, it could. But if the two measures are distinct, even if they were designed to capture the same information, this seems very-highly unlikely. This is a good example because it doesn’t imply an implementational mistake somewhere but it really suggests that something strange is going on. One should definitely track down what’s going on before moving on. Here we’re getting back to an earlier post on doing research without ground truth. It calls for looking into the problem quite carefully. So go back to the principle of “doing research” that started this post: Carefully inspect and critically evaluate your results. And yes, do this every time: it pays off. "],
["citing-your-methods-in-your-research-reports-and-publications.html", "Chapter 12 Citing your methods in your research reports (and publications)", " Chapter 12 Citing your methods in your research reports (and publications) citation algorithms research Knowledge is incredibly distributed, and we all use methods that are based on what other people developed. Knowing where the methods come from and citing them properly is essential for research. For example, suppose you use a method from graph theory (but it could be anything) to characterize correlations from a series of ROIs. One measure that is used is called global efficiency and relates to the “efficiency with which information can flow within a cluster of nodes”. Think of how gossip spreads really fast in some clusters of people and maybe less fast in some other types of organization; or how diseases spread, etc. If you use a measure like global efficiency you are not going to implement that from scratch, you’re going to find it somewhere else. So what exactly are you using? Think if it in terms of the following possibilites: I’m using global efficiency. I’m using global efficiency as defined in the Toolbox “The greatest toolbox available”. I’m using global efficiency , weighted version, as defined in the Toolbox “The greatest toolbox available”. I’m using the weithed global efficiency method developed here: Latora, V., &amp; Marchiori, M. (2001). Efficient behavior of small-word networks. Physical Review Letters, 87(19), 198701. as implemented in the Toolbox “The greatest toolbox available”. The way that you should think about this is in terms of \\(\\#4\\) above. The difference between \\(\\#3\\) and \\(\\#4\\) is not important here, but if you know a little about these methods, you should be able to see what I mean. Simply put, doing it this way makes you a better researcher. And when you develop, yourself, a new method that starts being applied by others, you will be thankful if they do the same. And no, you can’t just cite Wikipedia… Note: To be sure, things can get a little more convoluted sometimes and giving proper credit gets muddied. Do you need to state that “The greatest toolbox available” is implemented in Stan? Or more extremely, do you need to cite Newton if you use calculus? Or Leibniz because the calculus notation that you use was his idea? You get the point. But when in doubt just cite more, not less. "],
["within-and-between-participant-variability.html", "Chapter 13 Within- and between-participant variability", " Chapter 13 Within- and between-participant variability Data vary for lots of reasons. But in human and animal research, there are two forms of variability that are inherently distinct: within-participant and between-participant. Let’s explore some of the reasons to keep track of where variability comes from. (Before that let’s pause for a second to think why keeping track of these might appear to not matter. Suppose you have a group of observations \\(A\\) (of any kind) and a separate group of observartions B. So if you take the average of A, \\(\\bar{x}_A\\), and the average of \\(B\\), \\(\\bar{x}_B\\), the overall average \\(\\bar{x}_{A,B}\\) is the same as \\(\\frac{1}{2}( \\bar{x}_A + \\bar{x}_B)\\). This is simply because the “average of the averages” is the same as the “average of everything individually,” a convenient property of things that combine linearly.) Back to within-participant and between-participant variability. Suppose the observations \\(A\\) and \\(B\\) come from two separate sets of participants. We could see how different the “typical” responses to \\(A\\) and \\(B\\) are by calculating \\(\\bar{x}_A-\\bar{x}_B\\). Now suppose you have multiple observations \\(A\\) and \\(B\\) from the same participant, and that you have, as typical, multiple participants. In this case, you have a richer scenario because each participant has multiple data points for \\(A\\) and multiple data points for \\(B\\). You capitalize upon that nested structure in the following way. If you summarize the typical response to manipulation \\(A\\) as \\(\\bar{x}_A\\) and to manipulation \\(B\\) as \\(\\bar{x}_B\\) for subject \\(1\\), then \\(\\bar{x}_A-\\bar{x}_B\\) represents the difference score within that subject. You can do the same for all other participants. Now, if you are interested in how \\(A\\) and \\(B\\) differ, you can look at difference scores defined within participants and see how they are distributed across participants. What this does for you is to remove the subject effect, and allows you to focus on the treatment effect A vs. B. So if subject \\(1\\) is, for example, a “fast responder”&quot; (say fast reaction times in general), the fact that \\(B\\) is associated with a larger mean than \\(A\\) is not masked by across-individual variability in baseline responding. The figure below illustrates the situation. There’s a consistent treatment effect from \\(A\\) to \\(B\\) within subject, but that will only be clear if you eliminate across-participant variability. Figure 13.1: Within-subject differences studied across participants. \\(A\\) and \\(B\\) represent the two conditions of interest. The bottom gray line indicates the range of responding from \\(150\\) to \\(650\\) (say, reaction times in milliseconds), and arbitrary low, medium, and high respoding zones (L, M, and H). The small purple lines are individual trials, and the thicker line is the mean for that condition. Conclusion: When you have within-participant data think about the object of interest as difference scores. You have a much better way to estimate differences, so forget about \\(A\\) and \\(B\\) individually and think of \\(A-B\\). "],
["between-participant-estimates.html", "Chapter 14 Between-participant estimates", " Chapter 14 Between-participant estimates In Chapter 13 (Withing- and between-participant variability), we discussed the follwing idea: You might have a much better way to estimate differences, so forget about \\(A\\) and \\(B\\) individually and think of \\(A-B\\). But does this mean that you should not estimate \\(A\\) and/or \\(B\\)? The answer to this quesion is entirely determined by your subject matter, as it might be extremely important to estimate them individually. All we said in Chapter 13 is that you have a much better chance to estimate \\(A-B\\) in some scenarios. The field of statistics focuses on two main interrelated goals: (1) Estimating properties from data; and (2) Understanding the distributional properties of the estimates. Goal (2) usually takes place in the context of attempting to draw statistical inferences, but it needn’t be. So if understading properties about \\(A\\) is what your research calls for, go ahead and estimate it. But when you estimate it, never forget (2) even if your focus at that point is not on making inferences. In other words, estimate some property of \\(A\\), say \\(\\bar{x}_A\\), but don’t stop there. Study how \\(A\\) is distributed. Let’s work through a simple example. Consider that your goal is to estimate the representative value of \\(A\\) (something like the mean, median, or mode). Here, \\(A\\) is a strange object of interest: “the neural trajectory in time when someone is under threat in a given experiment”. The goal of using this unusual object is to illustrate that the principle applies very generally, no matter what property you want to understand from your data. When you estimate the representative value of \\(A\\), that’s a single value (or object in our case): that’s what statisticians call a point estimate. The problem is that while it might have great pratical and theoretical properties, the representative \\(A\\) is just that, a single thing. So you need to characterize how \\(A\\) is distributed. When you look at the distribution of your estimate of \\(A\\), you might find that it’s so variable as to defy utility (Figure 7.1). Figure 14.1: Estimates of a property of the data your interested in: in this case the trajectory shown in (A). (B) Based on the estimator that you’re using, the individual-level trajectories are incredibly variable. The (putatively) representative trajectory is clearly a very poor indicator of your object of study. In other words, based on data at hand, you do not have a good way of understanding what \\(A\\) is really about. But this is something that you should report, not hide. Why? Because that becomes a problem for a future study: designing a study in which \\(A\\) can be better understood. Based on your investigation, although you can’t estimate it well, you might gain multiple insights into doing this in the future. So this is very important. Your investigations are as much about “now” as the future of your research. "]
]
