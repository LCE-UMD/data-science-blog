# Understanding data, overfitting, and p values

`conceptual` `statistics` `machine learning` `cross validation`

If you're an engineer you might not be very familar with statistical inference and p values and the point of it all might be a little mysterious. At the same time, you might be very familiar with the issue of **overfitting data** and using something like training and test sets, cross-validation, and other schemes to investigate data. A neuroscientist will be less familar with those instead. But it turns out that they are closely related.

This post will be short and elaborated later (hopefully). Suppose you're an engineer and plot the data and get some 2-D data scatter plot. Then you decide to fit the data with a linear model (just fitting a line). But you also try a quadratic fit, a cubic fit, and even some fancy new spline method just published. It makes sense to try several methods because you don't know the data well (you're just studying it now). You then summarize how **all of the methods** behave, how well they fit the data.

**The paradox is that you can't do that in an experimental science situation**. You have to decide **in advance** which method you want to use and **only then** apply it to the data. But why? Isn't this too limiting?

The reason (this needs to be short) is that when you analyze a new dataset you usually don't have so much data that you can try a lot of methods, pick what you like most, and then apply it to the data that you really care about. You don't have data available like that. Because of this, if you try multiple methods you will be reporting what works best for your **specific dataset** which may **generalize very poorly** to new datasets. And what you want in general is to be able to generalize.

In the case of fit example above, perhaps the spline fit explains most of the variance in the data. But then it could be really overfitting your data and generalize very poorly to new datasets. You could find this out by testing it in a new dataset, that's easy enough. The problem is that you don't have data to test it with, so you're stuck. Data are so difficult to acquire in general that this approach is usually not possible. (HCP data and other large datasets are helpful in this respect, of course.)

Bottom line: you have to find some clever way to choose **all your analyses in advance**. Maybe you can use similar data, maybe simulated data, or just parsimony â€“ the simplest model possible, or the fewest assumptions.

To conclude, data analysis in an experimental setting is **not** a report of what happens to your data when you apply the set of most common types of fits/analyses available. It's what happens when you apply **one type of analysis** to **one dataset** that is supposed to be **representative of data in general**.

<u>Note 1</u>: Of course, it's always possible to perform **exploratory** data analysis in the context of methodological studies, but that's different.

<u>Note 2</u>: I never got to how this is related to *p values* and statistical inference. As it's commonly said, that's left as an exercise for the reader...
