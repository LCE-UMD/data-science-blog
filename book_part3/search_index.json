[
["index.html", "Part III: fMRI Data Analysis Chapter 1 Within- and between-participant variability", " Part III: fMRI Data Analysis Chapter 1 Within- and between-participant variability Data vary for lots of reasons. But in human and animal research, there are two forms of variability that are inherently distinct: within-participant and between-participant. Let’s explore some of the reasons to keep track of where variability comes from. (Before that let’s pause for a second to think why keeping track of these might appear to not matter. Suppose you have a group of observations \\(A\\) (of any kind) and a separate group of observartions B. So if you take the average of A, \\(\\bar{x}_A\\), and the average of \\(B\\), \\(\\bar{x}_B\\), the overall average \\(\\bar{x}_{A,B}\\) is the same as \\(\\frac{1}{2}( \\bar{x}_A + \\bar{x}_B)\\). This is simply because the “average of the averages” is the same as the “average of everything individually,” a convenient property of things that combine linearly.) Back to within-participant and between-participant variability. Suppose the observations \\(A\\) and \\(B\\) come from two separate sets of participants. We could see how different the “typical” responses to \\(A\\) and \\(B\\) are by calculating \\(\\bar{x}_A-\\bar{x}_B\\). Now suppose you have multiple observations \\(A\\) and \\(B\\) from the same participant, and that you have, as typical, multiple participants. In this case, you have a richer scenario because each participant has multiple data points for \\(A\\) and multiple data points for \\(B\\). You capitalize upon that nested structure in the following way. If you summarize the typical response to manipulation \\(A\\) as \\(\\bar{x}_A\\) and to manipulation \\(B\\) as \\(\\bar{x}_B\\) for subject \\(1\\), then \\(\\bar{x}_A-\\bar{x}_B\\) represents the difference score within that subject. You can do the same for all other participants. Now, if you are interested in how \\(A\\) and \\(B\\) differ, you can look at difference scores defined within participants and see how they are distributed across participants. What this does for you is to remove the subject effect, and allows you to focus on the treatment effect A vs. B. So if subject \\(1\\) is, for example, a “fast responder”&quot; (say fast reaction times in general), the fact that \\(B\\) is associated with a larger mean than \\(A\\) is not masked by across-individual variability in baseline responding. The figure below illustrates the situation. There’s a consistent treatment effect from \\(A\\) to \\(B\\) within subject, but that will only be clear if you eliminate across-participant variability. Figure 1.1: Within-subject differences studied across participants. \\(A\\) and \\(B\\) represent the two conditions of interest. The bottom gray line indicates the range of responding from \\(150\\) to \\(650\\) (say, reaction times in milliseconds), and arbitrary low, medium, and high respoding zones (L, M, and H). The small purple lines are individual trials, and the thicker line is the mean for that condition. Conclusion: When you have within-participant data think about the object of interest as difference scores. You have a much better way to estimate differences, so forget about \\(A\\) and \\(B\\) individually and think of \\(A-B\\). "],
["between-participant-estimates.html", "Chapter 2 Between-participant estimates", " Chapter 2 Between-participant estimates In Chapter 1 (Withing- and between-participant variability), we discussed the follwing idea: You might have a much better way to estimate differences, so forget about \\(A\\) and \\(B\\) individually and think of \\(A-B\\). But does this mean that you should not estimate \\(A\\) and/or \\(B\\)? The answer to this quesion is entirely determined by your subject matter, as it might be extremely important to estimate them individually. All we said in Chapter 13 is that you have a much better chance to estimate \\(A-B\\) in some scenarios. The field of statistics focuses on two main interrelated goals: (1) Estimating properties from data; and (2) Understanding the distributional properties of the estimates. Goal (2) usually takes place in the context of attempting to draw statistical inferences, but it needn’t be. So if understading properties about \\(A\\) is what your research calls for, go ahead and estimate it. But when you estimate it, never forget (2) even if your focus at that point is not on making inferences. In other words, estimate some property of \\(A\\), say \\(\\bar{x}_A\\), but don’t stop there. Study how \\(A\\) is distributed. Let’s work through a simple example. Consider that your goal is to estimate the representative value of \\(A\\) (something like the mean, median, or mode). Here, \\(A\\) is a strange object of interest: “the neural trajectory in time when someone is under threat in a given experiment”. The goal of using this unusual object is to illustrate that the principle applies very generally, no matter what property you want to understand from your data. When you estimate the representative value of \\(A\\), that’s a single value (or object in our case): that’s what statisticians call a point estimate. The problem is that while it might have great pratical and theoretical properties, the representative \\(A\\) is just that, a single thing. So you need to characterize how \\(A\\) is distributed. When you look at the distribution of your estimate of \\(A\\), you might find that it’s so variable as to defy utility (Figure 7.1). Figure 2.1: Estimates of a property of the data your interested in: in this case the trajectory shown in (A). (B) Based on the estimator that you’re using, the individual-level trajectories are incredibly variable. The (putatively) representative trajectory is clearly a very poor indicator of your object of study. In other words, based on data at hand, you do not have a good way of understanding what \\(A\\) is really about. But this is something that you should report, not hide. Why? Because that becomes a problem for a future study: designing a study in which \\(A\\) can be better understood. Based on your investigation, although you can’t estimate it well, you might gain multiple insights into doing this in the future. So this is very important. Your investigations are as much about “now” as the future of your research. "],
["brief-primer-on-fmri-analysis-of-task-experimental-designs-for-engineers-and-others-familiar-with-the-mathematical-background.html", "Chapter 3 Brief primer on fMRI analysis of task experimental designs (for engineers and others familiar with the mathematical background)", " Chapter 3 Brief primer on fMRI analysis of task experimental designs (for engineers and others familiar with the mathematical background) In what follows, the entire description refers to the analysis (1) at a single voxel time series from (2) a single participant. This is the basic building block from which activation maps for groups of participants are built. The defining property of an fMRI study employing a task is that there are well-defined periods during which experimental conditions are “On”. For example, consider a task involving viewing visual stimuli, listening to some sounds, and producing motor movements. Call these conditions \\(C_1\\), \\(C_2\\), and \\(C_3\\). What precisely defines a particular experiment is given by when and for how long each condition takes place during data acquisition. We often call a “run” an uninterrupted acquisition of fMRI data. Therefore, the following “boxcars” (timing diagram) define a specific experiment. They define On and Off periods for each of the experimental conditions. Figure 3.1: Timing diagram for conditions \\(C_1\\), \\(C_2\\), and \\(C_3\\) of a specific experiment. Each condition is thus represented by a vector containing \\(0\\) or \\(1\\) entries and with dimension 1 x t. FMRI data analysis is a simple application of multiple linear regression. \\(Y_t\\) is the time series data for a given voxel, the \\(X_i\\) are \\(p-1\\) predictors (also called explanatory variables, or regressors) defined for every time point \\(t\\), and \\(e\\) is the error term: \\(\\mathbf{Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots +\\beta_{p-1}X_{p-1}+\\epsilon}\\textrm(1)\\) or in matrix form: \\(\\mathbf{Y=X\\beta+\\epsilon}\\) with dimensions as follows: \\(\\mathbf{Y}:t\\times 1; \\mathbf{X}:t\\times(p-1); \\beta: (p-1)\\times 1; \\epsilon:n\\times t\\). The matrix \\(\\mathbf{X}\\) is often called the design matrix because it summarizes the experimental design. Note that it is a matrix with the first column being composed of the column vector \\(\\mathbf{1}\\) (this is obtained trivially because \\(\\mathbf{X_0}\\) is ommitted for convenience and is the column vector in question). It can be easily shown that the estimates \\(\\beta\\) are given by \\(\\beta=\\mathbf{(X&#39;X)^{-1}X&#39;Y}\\) where \\(\\mathbf{X&#39;}\\) is the transpose of \\(\\mathbf{X}\\). Thus, based on the data \\(Y_t\\) at each voxel separately, the goal of multiple regression is to fit a hyperplane that minimizes the residual \\((Y - \\hat{Y})^2\\), where \\(\\hat{Y}\\) is the predicted response. The predicted response is simply obtained by plugging in the regression coefficients estimated by, say, ordinary least squares. What does this mean? Your explanatory variables indicate when particular experimental conditions occur, so they define an idealized response that is “matched” to the observed data \\(Y_t\\). If your voxel turns On and Off as predicted by condition \\(C_i\\), then your regression coefficient \\(\\beta_i\\) will reflect that. Note that regressors directly based on the \\(C_i\\) vectors would be unrealistic because they would assume that a voxel’s response switches from Off to On (and On to Off) instantaneously. Therefore, the timing vector defined by \\(C_i\\) needs to be passed through a hemodynamic filter that takes into account the temporal properties of the BOLD signal. In other words, we need to convolve \\(C_i\\) with the hemodynamic impulse response function, which is something that has been well characterized (therefore is considered known). Accordingly, the idealized response (regressor) \\(X_i\\) is obtained by convolving \\(C_i\\) with the hemodynamic filter: \\(X_i = C_i * \\textrm{HRF}\\), where \\(*\\) is the convolution operator and \\(\\textrm{HRF}\\) is the hemodynamic filter. Conceptually, one should think that the goal is to estimate regression coefficients \\(\\beta_i\\) based on how they modulate the regressors such that the overall fit is optimized in the least-squares sense. Figure 3.2: The timing diagram specifies the condition \\(C_i\\), which is then used to determine the idealized response \\(X_i\\). The latter is simply the former convolved with the hemodynamic response function. The orange trace indicates the data at a particular voxel. It can be seen that it responds quite well to condition \\(C_i\\). For a specific individual, the exact procedure is repeated for all voxels in the brain. The space of voxels is often masked so as to consider only voxels that fall in gray matter, which is where neurons are found. Before ending this brief introduction, it’s useful to consider the geometry of least-squares regression, which will provide a deeper understanding of what we’re doing. Inspection of Equation (1) should lead to the intuition that once the \\(\\beta\\) coefficients are determined, \\(\\mathbf{\\hat{Y}}\\) is a “new” version of \\(\\mathbf{Y}\\) in a new coordinate system. Specifically, the coordinate system is given by the hyperplane spanned by the vectors \\(\\mathbf{X}_i\\). Moreover, \\(\\mathbf{\\hat{Y}}\\) is the predicted vector which is the projection of the original \\(\\mathbf{Y}\\) onto the space defined by the basis vectors \\(\\mathbf{X}_i\\). Overall the approach is quite elegant! \\[\\mathbf{\\hat{Y}=X\\hat{\\beta}}\\] Figure 3.3: Geometric interpretation of multiple regression as the orthogonal projection of the original data onto the space spanned by the explanatory variables. Essentially this is your data projected onto your experimental variables. Group analysis Group analysis is performed as follows. For a specific voxel v, regression coefficients are estimated for every condition and individual. This is considered the first level. Subsequently, a statistical test is performed that uses first-level regression coefficients. Standard tests such as a t test or ANOVA can be performed. The important point about the second-level statistical test is that it is performed across participants. In other words, from a given participant (and voxel), only a single regression coefficient \\(\\beta_i\\) is carried from the first to the second level. At the second level, one can then think of a mean coefficient per condition, together with its variability. In this manner, across-participant variability is factored into the statistics which is then considered to be a “random effects” analysis. In other words, participants are randomly sampled from the population when one samples them to participate in the experiment, and in this sense the statistical results speak to the properties of the population (which is the theoretical object of interest). "]
]
