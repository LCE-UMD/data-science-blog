---
title: "Multiple regression"
output: html_document
---
# Multiple regression as you never seen it before. Or the case for the importance of the assumption of independence.
 
## House price prediction

Imagine the following scenario, you work for a real estate agency and new house has just been made available on the market, how would you go about naming the price for that house? Luckily in this scenario you are not just any other real estate, but a statistics loving, extra diligent one. As such you have saved on record all the information about houses sold in your area for the past $10$ years. In your archives you have recorded the size of each house (in square feet), the number of bedrooms it has, the average income in the respective neighborhood, a subjective rating of how appealing the house is, the year in which it was sold, and how much it was sold for. How can you use all that information from your archives to best predict the value of the new house?

Price(Y)	| # of bedrooms	| Average income of the neighborhood	| Rating	| Year	| Area/sqf	| …	| House tax rate of this region
----------|---------------|-------------------------------------|---------|-------|-----------------|---|------------------------------
190,239	| 4	| 40,000	| 2.7	| 2012	| 2,642 | ... | 1.08%
…	| …	| …	| …	| …	| …	| …	| …
230,232	| 3	| 63,298	| 4.8	| 2016	| 1,324| ... | 2.01%

This problem could be mathematically formulated as following problem:

> Given a set of variables $X = (x_1, x_2, \cdots , x_n)$ and another variable $Y$, try to find the “relationship” between $X$ and $Y$.

In the house prediction problem above, $x1$ could be total number of bedrooms, $x2$ could be the average income of the neighborhood, xn could be house tax rate of this region and $Y$ could be the deal price at a certain year. Since we want to predict the price of a house as precise as possible, the “relationship” here could be simply interpreted as to find out the smallest difference between the predicted price and the deal price. So we can further formularized the above problem as:

> Given a set of variables $X = [x1, x2, \cdots , xn]$ and another variable $Y$, try to find a function $f:\mathcal{R}^n \rightarrow \mathcal{R}$ s.t “difference” between $f(X)$ and $Y$ is minimized.

Here, a problem might arise, how can one choose a proper function $f(X)$ ? A naive thought is to simply assume a linear relationship between $X$ and $Y$, i.e.

$$ f(X)=\sum{}{}\alpha_i x_i+ \alpha_0 $$
This linear assumption, even though simple enough, turns out to be very powerful and effective in solving many problems. Given this assumption, now the problem becomes:

> Given a set of variables $X = [x1, x2, \cdots , xn]$ and another variable $Y$, use data collected for variable $X$ and $Y$ to determine a set of values of coefficients $\alpha_0, \alpha_1, \cdots , \alpha_n$ s.t “difference” between $f(X)$ and $Y$ is minimized.

The problem we derived from the perspective of optimization is categorized as least-squares problem:

$$\underset{x}{min}\quad f_0(x)=\Vert Ax- b\Vert_2^2$$

It could be solved analytically with a solution of $x= (A^{\top}A)^{-1} A^{\top} b$. Also there are many efficient algorithms available to avoid the expensive computation of inverse of matrix.

Most people stop their analysis here when they happily got their final results: a set of coefficient values. Some might even use software packages as some magical black box that could produce a set of coefficient that could linearly fit the data. 

Known as multiple regression, statisticians would often use the aforementioned analysis to learn about the relationship between several independent or predictor variables and a dependent or criterion variable. So, if we go back to our house price prediction example. The relationships between the different predicting variables and the criterion can be described like this:

$$Price(Y) = 4.7 * \text{# of bedrooms} + 0.27* \text{Average income of the neighborhood} + 0.46 * \text{Rating}\\ + 0.2 * \text{Year} + 0.1 * \text{Area} + 0.24 * \text{House tax rate of this region}$$

What this equation is telling us is that when all other variables are being held as consents, every $4.7$ rooms added to the house results in a one-unit change in price of the house; let us say an added $100,000$ to the house total price.

Seeing this equation, our agent might reach the conclusion that due to the large coefficient the factor most significantly effecting the house prices in the area is the number of rooms that they have (when compared to the tiny $0.1$ coefficient of the house area for example). The effect of number of room is so central that for an increase of $4.7$ rooms drives the price of the house by another $100,000\$$.

This conclusion would be very wrong, and for a couple of reasons, we will call them the problems of collinearity and scaling. A house’s price is mainly determined by number of bedrooms but not determined by the house area? Who will design a house with many bedrooms but small area? In fact, how can the numbers of room grow without it affecting the area of the house? Can infect one factor in our equation change without it affecting other factors?

Of the two problems, that of scaling is simpler, and easier to resolve. A factor’s coefficient is a function of that factor’s range, so for large ranged variables the coefficient before it is likely to get a small value. That small value however, does NOT indicate that it has less impact in determining the value of $Y$. To avoid this problem, most data scientists tend to normalize data before conducting the regression analysis so that values of variable are in similar range.

The other problem, collinearity, refers to a situation when one of our predictor variables can be almost entirely predicted by one or a few of the other predicting variables. In our example, it is easy to see how, in many cases, the number of room it has could predict a house’s area. Similarly, if that region house taxation were based on its square feet size, the variable area would entirely predict that of tax. When two or more variables are highly correlated, they are in affect explaining the same phenomenon, and thus redundant. It is therefore a good practice to test for collinearity between the variables prior to designing the regression model.

However, even if variables are not entirely collinear, that does not mean we can treat them as orthogonal. If we go back to the verbal explanation of the regression equation, it is important to pay careful attention to the following part “when all other variables are being held as consents…” it means that we can refer to the unique effect a single variable if, and only if, changing that variable has no effect on the other variables. If it does, the equation should be treated as a whole.
