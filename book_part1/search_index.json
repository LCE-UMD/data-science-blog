[
["index.html", "Part I: Data Analysis Chapter 1 Multiple regression as you never seen it before. Or the case for the importance of the assumption of independence. 1.1 House price prediction", " Part I: Data Analysis Chapter 1 Multiple regression as you never seen it before. Or the case for the importance of the assumption of independence. 1.1 House price prediction Imagine the following scenario, you work for a real estate agency and new house has just been made available on the market, how would you go about naming the price for that house? Luckily in this scenario you are not just any other real estate, but a statistics loving, extra diligent one. As such you have saved on record all the information about houses sold in your area for the past \\(10\\) years. In your archives you have recorded the size of each house (in square feet), the number of bedrooms it has, the average income in the respective neighborhood, a subjective rating of how appealing the house is, the year in which it was sold, and how much it was sold for. How can you use all that information from your archives to best predict the value of the new house? Price(Y) # of bedrooms Average income of the neighborhood Rating Year Area/sqf … House tax rate of this region 190,239 4 40,000 2.7 2012 2,642 … 1.08% … … … … … … … … 230,232 3 63,298 4.8 2016 1,324 … 2.01% This problem could be mathematically formulated as following problem: Given a set of variables \\(X = (x_1, x_2, \\cdots , x_n)\\) and another variable \\(Y\\), try to find the “relationship” between \\(X\\) and \\(Y\\). In the house prediction problem above, \\(x1\\) could be total number of bedrooms, \\(x2\\) could be the average income of the neighborhood, xn could be house tax rate of this region and \\(Y\\) could be the deal price at a certain year. Since we want to predict the price of a house as precise as possible, the “relationship” here could be simply interpreted as to find out the smallest difference between the predicted price and the deal price. So we can further formularized the above problem as: Given a set of variables \\(X = [x1, x2, \\cdots , xn]\\) and another variable \\(Y\\), try to find a function \\(f:\\mathcal{R}^n \\rightarrow \\mathcal{R}\\) s.t “difference” between \\(f(X)\\) and \\(Y\\) is minimized. Here, a problem might arise, how can one choose a proper function \\(f(X)\\) ? A naive thought is to simply assume a linear relationship between \\(X\\) and \\(Y\\), i.e. \\[ f(X)=\\sum{}{}\\alpha_i x_i+ \\alpha_0 \\] This linear assumption, even though simple enough, turns out to be very powerful and effective in solving many problems. Given this assumption, now the problem becomes: Given a set of variables \\(X = [x1, x2, \\cdots , xn]\\) and another variable \\(Y\\), use data collected for variable \\(X\\) and \\(Y\\) to determine a set of values of coefficients \\(\\alpha_0, \\alpha_1, \\cdots , \\alpha_n\\) s.t “difference” between \\(f(X)\\) and \\(Y\\) is minimized. The problem we derived from the perspective of optimization is categorized as least-squares problem: \\[\\underset{x}{min}\\quad f_0(x)=\\Vert Ax- b\\Vert_2^2\\] It could be solved analytically with a solution of \\(x= (A^{\\top}A)^{-1} A^{\\top} b\\). Also there are many efficient algorithms available to avoid the expensive computation of inverse of matrix. Most people stop their analysis here when they happily got their final results: a set of coefficient values. Some might even use software packages as some magical black box that could produce a set of coefficient that could linearly fit the data. Known as multiple regression, statisticians would often use the aforementioned analysis to learn about the relationship between several independent or predictor variables and a dependent or criterion variable. So, if we go back to our house price prediction example. The relationships between the different predicting variables and the criterion can be described like this: \\[Price(Y) = 4.7 * \\text{# of bedrooms} + 0.27* \\text{Average income of the neighborhood} + 0.46 * \\text{Rating}\\\\ + 0.2 * \\text{Year} + 0.1 * \\text{Area} + 0.24 * \\text{House tax rate of this region}\\] What this equation is telling us is that when all other variables are being held as consents, every \\(4.7\\) rooms added to the house results in a one-unit change in price of the house; let us say an added \\(100,000\\) to the house total price. Seeing this equation, our agent might reach the conclusion that due to the large coefficient the factor most significantly effecting the house prices in the area is the number of rooms that they have (when compared to the tiny \\(0.1\\) coefficient of the house area for example). The effect of number of room is so central that for an increase of \\(4.7\\) rooms drives the price of the house by another \\(100,000\\$\\). This conclusion would be very wrong, and for a couple of reasons, we will call them the problems of collinearity and scaling. A house’s price is mainly determined by number of bedrooms but not determined by the house area? Who will design a house with many bedrooms but small area? In fact, how can the numbers of room grow without it affecting the area of the house? Can infect one factor in our equation change without it affecting other factors? Of the two problems, that of scaling is simpler, and easier to resolve. A factor’s coefficient is a function of that factor’s range, so for large ranged variables the coefficient before it is likely to get a small value. That small value however, does NOT indicate that it has less impact in determining the value of \\(Y\\). To avoid this problem, most data scientists tend to normalize data before conducting the regression analysis so that values of variable are in similar range. The other problem, collinearity, refers to a situation when one of our predictor variables can be almost entirely predicted by one or a few of the other predicting variables. In our example, it is easy to see how, in many cases, the number of room it has could predict a house’s area. Similarly, if that region house taxation were based on its square feet size, the variable area would entirely predict that of tax. When two or more variables are highly correlated, they are in affect explaining the same phenomenon, and thus redundant. It is therefore a good practice to test for collinearity between the variables prior to designing the regression model. However, even if variables are not entirely collinear, that does not mean we can treat them as orthogonal. If we go back to the verbal explanation of the regression equation, it is important to pay careful attention to the following part “when all other variables are being held as consents…” it means that we can refer to the unique effect a single variable if, and only if, changing that variable has no effect on the other variables. If it does, the equation should be treated as a whole. "],
["how-can-we-do-data-analysis-when-theres-no-ground-truth.html", "Chapter 2 How can we do data analysis when there’’s no “ground truth”?", " Chapter 2 How can we do data analysis when there’’s no “ground truth”? We often investigate novel imaging datasets with sophisticated algorithms. These might be something that we just developed or some technique that exists in the literature and has been applied in other domains. In any case, it’s something that isn’t trivial and will involved quite a bit of data processing or manipulation. You get a new result, now what? Maybe the result makes some sense, or maybe it’s not entirely clear. Can you trust the results? If it the results make sense, should you trust the results? The answer is no, don’t trust it – yet. There are more ways to get something wrong than to get it right. So just because of this it will be quite possible that something went wrong. Not just in terms of programming but also conceptually. Maybe you thought what you were doing made sense, but it actually doesn’t and you haven’t realized it. Maybe it will take quite some time for you to understand your method more deeply. What to do? First, apply your method to something that is simple and that you believe you can interpret. Try to break your method to understand it better. Do this for a good while, by the way. Next, apply it to something that is not simple but that is well understood, or relatively well understood. Let me give you an example here. Suppose you want to study something novel and challenging, like our emoprox I/II data. You apply your algorithm and get some result. Maybe you don’t even have much experience with brain data, so interpreting will be quite hard. So, now apply it to HCP data, for example, which has been studied extensively in many different ways. So by usign this better understood dataset and now understanding your method a little better (and fixing any implementation and hopefully minor conceptual mistakes), you can now return to your original problem of interest. There’s still a lot to understand but at least now you know something about the variability of the measures that you’re using, how they might be distributed, and so on. A lot of research is done – obviously – without any ground truth. That is an immense challenge and even very skilled people technically struggle when studying new problems. That’s because both the data and their methods are not fully understood. This is a perfect recipe for things to go wrong, at least partly wrong. So one has to incrementally bootstrap one’s understanding of both the data and the method along the ways outlined here. Obviously, the example above is arbitrary (not entirely by the way). Maybe you are studying HCP data for the first time yourself with your method. So pick something simpler than HCP and apply the logic above. If you don’t know what data to pick, or it’s not obvious what could be used, an excellent choice is to create synthetic data yourself. You should contruct data with certain properties and understand how your method/measures behaves. Again, think of the distribution of results that you get; how do they vary? Synthetic data is seldom used, but it’s hugely useful. So think of how to structure your workflow so as to incorporate that in a more seamless manner. I’m going to stop here. There’s a lot more to discuss about this, but I wanted to post this simple version first. Maybe one day I’ll extend it based on questions or suggestions. "],
["finishingsubmitting-a-paper-in-the-lab.html", "Chapter 3 Finishing/submitting a paper in the lab", " Chapter 3 Finishing/submitting a paper in the lab lab procedures etc As I’ve discussed recently, from now on we need to archive data when a paper is submitted for publication. At a minimum the following is required (and will be updated as we converge on a more established procedure): File the latest version of the paper in bioRxiv or most relevant repository and make sure that the lab’s website has a link to that version File all data (fMRI and behavior) in our system (not in a personal account/directory) Archive all data in OpenfMRI or other acceptable public format File all the processing programs, including versions used File all illustration figures File all data figures, including scripts/programs used to generate them. In case the figure involved manual decisions (selection of slices, etc.) this has to be thoroughly documented. Item #6 is particularly important and needs executable by someone other than yourself. In a typical case, an RA will work with you on this. In other words, it no longer suffices to have “all the data backed up”. We need to collectively work on this so as to attain better levels of reproducibility: can someone actually reproduce the work that you did? Finally, not that while before this was encouraged, now it is absolutely mandatory and part of working in the lab. Science is moving fast and we’ve been lagging behind for too long. "],
["understanding-data-overfitting-and-p-values.html", "Chapter 4 Understanding data, overfitting, and p values", " Chapter 4 Understanding data, overfitting, and p values conceptual statistics machine learning cross validation If you’re an engineer you might not be very familar with statistical inference and p values and the point of it all might be a little mysterious. At the same time, you might be very familiar with the issue of overfitting data and using something like training and test sets, cross-validation, and other schemes to investigate data. A neuroscientist will be less familar with those instead. But it turns out that they are closely related. This post will be short and elaborated later (hopefully). Suppose you’re an engineer and plot the data and get some 2-D data scatter plot. Then you decide to fit the data with a linear model (just fitting a line). But you also try a quadratic fit, a cubic fit, and even some fancy new spline method just published. It makes sense to try several methods because you don’t know the data well (you’re just studying it now). You then summarize how all of the methods behave, how well they fit the data. The paradox is that you can’t do that in an experimental science situation. You have to decide in advance which method you want to use and only then apply it to the data. But why? Isn’t this too limiting? The reason (this needs to be short) is that when you analyze a new dataset you usually don’t have so much data that you can try a lot of methods, pick what you like most, and then apply it to the data that you really care about. You don’t have data available like that. Because of this, if you try multiple methods you will be reporting what works best for your specific dataset which may generalize very poorly to new datasets. And what you want in general is to be able to generalize. In the case of fit example above, perhaps the spline fit explains most of the variance in the data. But then it could be really overfitting your data and generalize very poorly to new datasets. You could find this out by testing it in a new dataset, that’s easy enough. The problem is that you don’t have data to test it with, so you’re stuck. Data are so difficult to acquire in general that this approach is usually not possible. (HCP data and other large datasets are helpful in this respect, of course.) Bottom line: you have to find some clever way to choose all your analyses in advance. Maybe you can use similar data, maybe simulated data, or just parsimony – the simplest model possible, or the fewest assumptions. To conclude, data analysis in an experimental setting is not a report of what happens to your data when you apply the set of most common types of fits/analyses available. It’s what happens when you apply one type of analysis to one dataset that is supposed to be representative of data in general. Note 1: Of course, it’s always possible to perform exploratory data analysis in the context of methodological studies, but that’s different. Note 2: I never got to how this is related to p values and statistical inference. As it’s commonly said, that’s left as an exercise for the reader… "],
["interpreting-results-humility-pays-off.html", "Chapter 5 Interpreting results: humility pays off", " Chapter 5 Interpreting results: humility pays off In everyone’s research, programming mistakes (bugs) happen with higher frequency than we would want. This is inevitable given that programming is an inexact science – and it takes long time to get good at it. Not to mention putting together code that is partly yours and uses multiple functions developed by others. It becomes criticial then to do the following: Carefully inspect and critically evaluate your results. I’ve noticed that I often detect problems when they should have been spotted by others. Sometimes the detection is rather easy, which still surprises me. This leads me to think that learning to be a good researcher depends on working hard on the statement of the previous paragraph. Let me give a concrete example, though the example itself is not important. Suppose you’re learning a new technique, something like network/graph theory. There are a dozen basic measures (efficiency, modularity, characteristic path length, etc.), and you’re learning about them. Then you compute these measures on a data sample, one measure per individual (and for each condition). How are the measures similar or different? You then compute the correlation between the measures across participants. What would it mean for the correlation between two measures (say, characteristic path length and efficiency for example) to be exactly 1? How about 0? Two different measures cannot be correlated 100% unless they are mathematically and implementationally the exact same thing. In a real sample, they also cannot be correlated 0%, as sampling variability and noise will always produce a correlation that is different from zero, even if very small. So if you observed these values it should be a very strong indication that something is wrong (with p approaching 1…). Figure 5.1: Correlation matrix. The diagonals should be \\(1.0\\) by definition, but can any off-diagonal assume \\(1.0\\)? What if you observed the correlation between two measures as \\(0.99\\)? Could this happen? Figure 5.2: Correlation scatter. The two measures are correlated \\(0.9983\\). Logically speaking, it could. But if the two measures are distinct, even if they were designed to capture the same information, this seems very-highly unlikely. This is a good example because it doesn’t imply an implementational mistake somewhere but it really suggests that something strange is going on. One should definitely track down what’s going on before moving on. Here we’re getting back to an earlier post on doing research without ground truth. It calls for looking into the problem quite carefully. So go back to the principle of “doing research” that started this post: Carefully inspect and critically evaluate your results. And yes, do this every time: it pays off. "],
["citing-your-methods-in-your-research-reports-and-publications.html", "Chapter 6 Citing your methods in your research reports (and publications)", " Chapter 6 Citing your methods in your research reports (and publications) citation algorithms research Knowledge is incredibly distributed, and we all use methods that are based on what other people developed. Knowing where the methods come from and citing them properly is essential for research. For example, suppose you use a method from graph theory (but it could be anything) to characterize correlations from a series of ROIs. One measure that is used is called global efficiency and relates to the “efficiency with which information can flow within a cluster of nodes”. Think of how gossip spreads really fast in some clusters of people and maybe less fast in some other types of organization; or how diseases spread, etc. If you use a measure like global efficiency you are not going to implement that from scratch, you’re going to find it somewhere else. So what exactly are you using? Think if it in terms of the following possibilites: I’m using global efficiency. I’m using global efficiency as defined in the Toolbox “The greatest toolbox available”. I’m using global efficiency , weighted version, as defined in the Toolbox “The greatest toolbox available”. I’m using the weithed global efficiency method developed here: Latora, V., &amp; Marchiori, M. (2001). Efficient behavior of small-word networks. Physical Review Letters, 87(19), 198701. as implemented in the Toolbox “The greatest toolbox available”. The way that you should think about this is in terms of \\(\\#4\\) above. The difference between \\(\\#3\\) and \\(\\#4\\) is not important here, but if you know a little about these methods, you should be able to see what I mean. Simply put, doing it this way makes you a better researcher. And when you develop, yourself, a new method that starts being applied by others, you will be thankful if they do the same. And no, you can’t just cite Wikipedia… Note: To be sure, things can get a little more convoluted sometimes and giving proper credit gets muddied. Do you need to state that “The greatest toolbox available” is implemented in Stan? Or more extremely, do you need to cite Newton if you use calculus? Or Leibniz because the calculus notation that you use was his idea? You get the point. But when in doubt just cite more, not less. "],
["the-magic-of-averaging.html", "Chapter 7 The magic of averaging", " Chapter 7 The magic of averaging central limit theorem signal-to-noise ratio estimation [** Very brief post about averaging] In biological systems (and many others) we often deal with very noisy data. When first handling this type of data people are often confused with the following fact: Data from a single trial might (appear to) be “meaningless”. The same thing can be even said for data for a single participant – even when averaging trials within participant. For example, we rarely if ever will look at an individual’s fMRI data; it’s just too noisy (N.B.: this might appear contradict another post but it doesn’t.). Consider the figure below with fMRI responses to a brief stimulus. These are shown for two subjects (separately), for individual trials (labeled “\\(1\\)”), for averages of \\(4\\) trials (“\\(4\\)”), and so on. The single trials are a complete mess and look like noise! If you looked at one, or a few, you might think that the situation is hopeless. But with the beauty of the central limit theorem and the law of large numbers we see that the underlying shape is there with as few as \\(16\\) trials, and clearly with \\(64\\). And the standard deviation at the peak decreases with the \\(sqrt(N)\\) trials, so there are diminishing returns after some point. Conclusion: don’t be afraid when your data looks messy around the mean… "]
]
